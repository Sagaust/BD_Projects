{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sagaust/BD_Projects/blob/main/AfricanPedia_Youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnIzuxckjfm5"
      },
      "outputs": [],
      "source": [
        "class YouTubeOperations:\n",
        "    API_KEY = ''\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.channels_ids = []\n",
        "        self.youtube_urls = self.urls_to_id()\n",
        "        with open(\"metadata.csv\", mode='w', newline='', encoding='utf-8') as meta:\n",
        "            writer = csv.writer(meta)\n",
        "            writer.writerow([\n",
        "                \"Video_ID\",\n",
        "                \"Channel_ID\",\n",
        "                \"Video_Title\",\n",
        "                \"Description\",\n",
        "                \"Tags\",\n",
        "                \"Likes\",\n",
        "                \"Views\",\n",
        "                \"Comments\",\n",
        "                \"Published_At\"\n",
        "            ])\n",
        "\n",
        "    def urls_to_id(self):\n",
        "        df = pd.read_excel(self.path, usecols=[0]).rename({\"Url\": \"URL\"}, axis=1)\n",
        "        df[\"URL\"] = df[\"URL\"].apply(self.extract_youtube_video_id)\n",
        "        df = df.drop_duplicates().reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    def extract_youtube_video_id(self, url):\n",
        "        \"\"\"Extract the video ID from a YouTube URL.\"\"\"\n",
        "        video_id = None\n",
        "        match = re.search(r\"(v=|youtu.be/|embed/)([a-zA-Z0-9_-]{11})\", url)\n",
        "        if match:\n",
        "            video_id = match.group(2)\n",
        "        return video_id\n",
        "\n",
        "    def fetch_video_metadata(self, video_id, api_key=API_KEY):\n",
        "        \"\"\"Fetch video metadata such as title, description, and channel details.\"\"\"\n",
        "        try:\n",
        "            youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "            request = youtube.videos().list(\n",
        "                part='snippet,contentDetails,statistics',\n",
        "                id=video_id\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            video_data = response[\"items\"][0][\"snippet\"]\n",
        "            statistics = response[\"items\"][0][\"statistics\"]\n",
        "\n",
        "            self.channels_ids.append(video_data['channelId'])\n",
        "\n",
        "            print(video_id)\n",
        "\n",
        "            values = {\n",
        "                'video_id': video_id,\n",
        "                'channelId': video_data['channelId'],\n",
        "                'title': video_data['title'],\n",
        "                'description': video_data['description'] if \"description\" in video_data else None,\n",
        "                'tags': \" \".join(video_data[\"tags\"]) if \"tags\" in video_data else None,\n",
        "                'likeCount': statistics[\"likeCount\"] if \"likeCount\" in statistics else None,\n",
        "                'viewCount': statistics[\"viewCount\"] if \"viewCount\" in statistics else None,\n",
        "                'commentCount': statistics[\"commentCount\"] if \"commentCount\" in statistics else None,\n",
        "                'publishedAt': video_data['publishedAt'] if \"publishedAt\" in video_data else None,\n",
        "            }\n",
        "\n",
        "            with open(\"metadata.csv\", mode='a+', newline='', encoding='utf-8') as meta:\n",
        "                meta_write = csv.writer(meta)\n",
        "                meta_write.writerow(list(values.values()))\n",
        "\n",
        "            return values[\"channelId\"]\n",
        "        except HttpError as e:\n",
        "            print(f\"An error occurred fetching metadata for video ID {video_id}: {e}\")\n",
        "            return None\n",
        "        except IndexError as ie:\n",
        "            print(f\"Could not generate items: {ie}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_youtube_comments(self, video_id, api_key=API_KEY):\n",
        "        \"\"\"Fetch top-level comments, replies, and commenter information.\"\"\"\n",
        "        comments = []\n",
        "        replies = []\n",
        "        try:\n",
        "            youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "            request = youtube.commentThreads().list(\n",
        "                part='snippet,replies',\n",
        "                videoId=video_id,\n",
        "                maxResults=100,\n",
        "                textFormat='plainText'\n",
        "            )\n",
        "\n",
        "            while request:\n",
        "                response = request.execute()\n",
        "                for item in response['items']:\n",
        "                    comment = item['snippet']['topLevelComment']['snippet']\n",
        "                    comment_info = {\n",
        "                        'commentID': item['snippet']['topLevelComment']['id'],\n",
        "                        'videoID': video_id,\n",
        "                        'author': comment['authorDisplayName'],\n",
        "                        'authorChannelId': comment['authorChannelId']['value'] if 'authorChannelId' in comment else None,\n",
        "                        'text': comment['textDisplay'],\n",
        "                        'likeCount': comment['likeCount'],\n",
        "                        'publishedAt': comment['publishedAt'],\n",
        "                    }\n",
        "                    # Check for replies\n",
        "                    if 'replies' in item:\n",
        "                        # print(item)\n",
        "                        for reply in item['replies']['comments']:\n",
        "                            reply_info = {\n",
        "                                'replyID': reply[\"id\"].split(\".\")[1],\n",
        "                                'CommentID': reply[\"id\"].split(\".\")[0],\n",
        "                                'author': reply['snippet']['authorDisplayName'],\n",
        "                                'authorChannelId': reply['snippet']['authorChannelId']['value'] if 'authorChannelId' in reply['snippet'] else None,\n",
        "                                'text': reply['snippet']['textDisplay'],\n",
        "                                'likeCount': reply['snippet']['likeCount'],\n",
        "                                'publishedAt': reply['snippet']['publishedAt']\n",
        "                            }\n",
        "                        replies.append(reply_info)\n",
        "\n",
        "                    comments.append(comment_info)\n",
        "\n",
        "                # Get next page of comments if available\n",
        "                if 'nextPageToken' in response:\n",
        "                    request = youtube.commentThreads().list_next(request, response)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        except HttpError as e:\n",
        "            print(f\"An error occurred fetching comments for video ID {video_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "        return comments, replies\n",
        "\n",
        "    def save_data_to_csv(self, video_id):\n",
        "        \"\"\"Save video metadata, comments, and replies to a CSV file.\"\"\"\n",
        "        comments_path = f\"./comments/{video_id}_comments.csv\"\n",
        "        replies_path = f\"./replies/{video_id}_replies.csv\"\n",
        "        my_file = Path(comments_path)\n",
        "        comments_column_names = [\n",
        "            \"CommentID\", \"Video_ID\", \"Commenter_Name\", \"Commenter_Channel_ID\", \"Comment_Text\", \"Likes\", \"Published_At\"\n",
        "        ]\n",
        "        replies_column_names = [\n",
        "            \"ReplyID\", \"CommentID\", \"Commenter_Name\", \"Commenter_Channel_ID\", \"Comment_Text\", \"Likes\", \"Published_At\"\n",
        "        ]\n",
        "\n",
        "        channel_id = self.fetch_video_metadata(video_id)\n",
        "\n",
        "\n",
        "        comments, replies = self.fetch_youtube_comments(video_id)\n",
        "\n",
        "        if not comments:\n",
        "            return None\n",
        "        comments_df = pd.DataFrame(comments)\n",
        "        comments_df.columns = comments_column_names\n",
        "        comments_df.to_csv(comments_path, index=False)\n",
        "        replies_df = pd.DataFrame(replies)\n",
        "        replies_df.columns = replies_column_names\n",
        "        replies_df.to_csv(replies_path, index=False)\n",
        "\n",
        "        comments_df.to_csv(comments_path, index=False)\n",
        "        replies_df.to_csv(replies_path, index=False)\n",
        "\n",
        "    def get_channel_videos(self, channel_id):\n",
        "        videos = scrapetube.get_channel(channel_id)\n",
        "        channel_dict = {\"videoId\": [], \"videoTitle\": []}\n",
        "\n",
        "        for video in videos:\n",
        "            channel_dict[\"videoId\"].append(video['videoId'])\n",
        "            channel_dict[\"videoTitle\"].append(video['title']['runs'][0]['text'])\n",
        "            # break\n",
        "        channel_df = pd.DataFrame(channel_dict)\n",
        "        channel_df[\"channelId\"] = [channel_id] * channel_df.shape[0]\n",
        "        channel_df = channel_df[[\"channelId\", \"videoId\", \"videoTitle\"]]\n",
        "        channel_df.to_csv(f\"./other_videos_from_channels/{channel_id}.csv\", index=False)\n",
        "\n",
        "    def round_up(self):\n",
        "        video_ids = self.youtube_urls[\"URL\"]\n",
        "        for video_id in video_ids:\n",
        "            try:\n",
        "                self.save_data_to_csv(video_id)\n",
        "            except:\n",
        "                continue\n",
        "        channel_ids = pd.DataFrame(self.channels_ids, columns=[\"channelId\"])\n",
        "        channel_ids = channel_ids.drop_duplicates().dropna()\n",
        "        print(\"Done with Videos, Comments, and Replies\")\n",
        "        for channel_id in channel_ids[\"channelId\"]: # use the self.channel_ids instead\n",
        "            self.get_channel_videos(channel_id)\n",
        "        print(\"Ces't finis\")"
      ]
    }
  ]
}